{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tunable_agents import utility, agent\n",
    "import utils\n",
    "import gin\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tunable_agents.environments.gathering_env import gathering_env\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "#gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "#tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dir = \"C:/Users/maler/Google Drive/Personale/Università/Master/Tesi/Code/tunable-agents-MORL/configs/\"\n",
    "\n",
    "gin_files = [configs_dir + \"envs/cumulative_linear_dual_threshold_env.gin\", configs_dir + \"qnets/128_128_64_model.gin\"]\n",
    "#gin_files = [configs_dir + \"envs/fixed_env.gin\", configs_dir + \"qnets/128_128_64_model.gin\"]\n",
    "gin_bindings = []\n",
    "utility.load_gin_configs(gin_files, gin_bindings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"C:/Users/maler/Federico/Università/Master/Tesi/experiments_results/\"\n",
    "root_path = \"C:/Users/maler/Federico/Università/Master/Tesi/new_results/\"\n",
    "experiment = \"128_128_64_model-cum_linear_dual_threshold_env-replication-1\"\n",
    "\n",
    "experiment_dir = os.path.join(root_path, experiment)\n",
    "model_dir = os.path.join(experiment_dir, 'model')\n",
    "model_path = os.path.join(model_dir, 'dqn_model.h5')\n",
    "\n",
    "\n",
    "env = utility.create_environment()\n",
    "tf_agent = agent.DQNAgent(epsilon=lambda: 0.1, obs_spec=env.observation_spec())\n",
    "\n",
    "#tf_agent.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf_agent.greedy_policy(time_step.observation)\n",
    "action = 2\n",
    "time_step = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7), dpi=100)\n",
    "image = time_step.observation['state_obs'][:, :, 6:]\n",
    "plt.imshow(image)\n",
    "plt.title('Utility: {}    Action: {}'.format(time_step.reward, action))\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "if time_step.is_last():\n",
    "    print('END EPISODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env._utility_func.utility_repr)\n",
    "print(env._cumulative_rewards)\n",
    "print(env._prev_step_utility)\n",
    "print(env._interests)"
   ]
  },
  {
   "source": [
    "# Agent training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_step(env, tf_agent: agent.DQNAgent,\n",
    "                    replay_memory: agent.ReplayMemory, reward_tracker: agent.RewardTracker,\n",
    "                    collect_episodes: int) -> None:\n",
    "    \"\"\"Samples transitions with the given Driver.\"\"\"\n",
    "    if not collect_episodes:\n",
    "        return\n",
    "\n",
    "    for _ in range(collect_episodes):\n",
    "        # Reset env\n",
    "        ts = env.reset()\n",
    "        observations = ts.observation\n",
    "\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = tf_agent.epsilon_greedy_policy(observations, training=True)\n",
    "            ts = env.step(action)\n",
    "            next_obs, reward, done = ts.observation, ts.reward, ts.is_last()\n",
    "\n",
    "            replay_memory.append((observations, action, reward, next_obs, done))\n",
    "            observations = next_obs\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "        reward_tracker.append(episode_reward)\n",
    "\n",
    "    return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = agent.ReplayMemory(maxlen=6000)\n",
    "reward_tracker = agent.RewardTracker(maxlen=100)\n",
    "\n",
    "collection_step(env, tf_agent, replay_memory, reward_tracker, collect_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = replay_memory.sample(32)\n",
    "states, actions, rewards, next_states, dones = experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_Q_values = tf_agent._target_model(\n",
    "    [next_states[\"state_obs\"]] +\n",
    "    [next_states[key] for key in sorted(next_states.keys() - [\"state_obs\"])])\n",
    "\n",
    "max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "target_Q_values = (rewards + (1 - dones) * tf_agent._gamma * max_next_Q_values)\n",
    "target_Q_values = target_Q_values.reshape(-1, 1)  # Make column vector\n",
    "\n",
    "# Mask to only consider action taken\n",
    "mask = tf.one_hot(actions, tf_agent._output_size)  #pylint: disable=no-value-for-parameter\n",
    "# Compute loss and gradient for predictions on 'states'\n",
    "with tf.GradientTape() as tape:\n",
    "    all_Q_values = tf_agent._model([states[\"state_obs\"]] +\n",
    "                                [states[key] for key in sorted(states.keys() - [\"state_obs\"])],\n",
    "                                training=True)\n",
    "    Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "    loss = tf.reduce_mean(tf_agent._loss_fn(target_Q_values, Q_values))\n",
    "grads = tape.gradient(loss, tf_agent._model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(grads[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actions)\n",
    "print(rewards)\n",
    "print(next_Q_values.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python395jvsc74a57bd0f79d2fcbf4c1e2dffa334de094e7255b26cc141c163e7ad958309f2cee9fb6ba",
   "display_name": "Python 3.9.5 64-bit ('master_thesis_venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}