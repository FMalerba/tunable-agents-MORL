# Experiment
#train_eval.experiment_name = 'gathering_replication' # Used to name the folder for logs/summaries/checkpoints
#train_eval.env_id = 'gathering_replication'

# Agent.
gamma = 0.99
create_agent.agent_class = 'replication_study'
create_agent.learning_rate = 1e-4
train_eval.num_steps = 2
DqnAgent.target_update_tau = 0.05
DqnAgent.target_update_period = 5
DqnAgent.gamma = %gamma
DqnAgent.reward_scale_factor = 1.0
DqnAgent.debug_summaries = False
DqnAgent.summarize_grads_and_vars = False

# Environment
create_environment.game = 'gathering'
GatheringWrapper.gamma = %gamma
GatheringWrapper.history_size = 3

# Replay Buffer
create_replay_buffer.max_length = 50_000    # RB capacity

# Decaying Epsilon for the Epsilon-Greedy policy
decaying_epsilon.decay_type = 'linear'
linear_decay.initial_epsilon = 0.9
linear_decay.final_epsilon = 0.05
linear_decay.decay_time = 10_000

# Running and Training process
initial_collection.collect_episodes = 50
train_eval.num_iterations = 50_000
train_eval.collect_episodes_per_epoch = 1
train_eval.train_steps_per_epoch = 100
train_eval.num_eval_episodes = 1000
train_eval.batch_size = 32

# Checkpointing and Evaluation
train_eval.train_checkpoint_interval = 1000
train_eval.policy_checkpoint_interval = 1000
train_eval.rb_checkpoint_interval = 1000
train_eval.eval_interval = 500




