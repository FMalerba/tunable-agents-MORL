# Experiment
train_eval.training_id = 'replication'                # Used to name the folder for logs/summaries/checkpoints

# Running and Training process
initial_step/collection_step.collect_episodes = 50
collect_episodes_per_epoch = 1
num_eval_episodes = 1000
train_eval.num_iterations = 200_000
train_eval.num_eval_episodes = %num_eval_episodes
collection_step.collect_episodes = %collect_episodes_per_epoch
training_step.batch_size = 32
training_step.train_steps = 1

# Agent.
DQNAgent.learning_rate = 1e-4
DQNAgent.gamma = 1
train_eval.target_update_period = 50

# Replay Buffer
train_eval.replay_size = 6_000    # RB capacity

# Reward Tracker
average_reward_window = 100
RewardTracker.maxlen = %average_reward_window
plot_learning_curve.average_reward_window = %average_reward_window

# Decaying Epsilon for the Epsilon-Greedy policy
decaying_epsilon.decay_type = 'linear'
linear_decay.initial_epsilon = 1.0
linear_decay.final_epsilon = 0.01
linear_decay.decay_time = 50_000

# Checkpointing and Evaluation
train_eval.checkpoint_interval = 4_000
train_eval.eval_interval = 2000
